# Telegraf Configuration
#
# Telegraf is entirely plugin driven. All metrics are gathered from the
# declared inputs, and sent to the declared outputs.
#
# Plugins must be declared in here to be active.
# To deactivate a plugin, comment out the name and any variables.
#
# Use 'telegraf -config telegraf.conf -test' to see what metrics a config
# file would generate.
#
# Environment variables can be used anywhere in this config file, simply prepend
# them with $. For strings the variable must be within quotes (ie, "$STR_VAR"),
# for numbers and booleans they should be plain (ie, $INT_VAR, $BOOL_VAR)


# Global tags can be specified here in key="value" format.
[global_tags]
  # dc = "us-east-1" # will tag all metrics with dc=us-east-1
  # rack = "1a"
  ## Environment variables can be used as tags, and throughout the config file
  # user = "$USER"
  project_name= "$OS_PROJECT"


# Configuration for telegraf agent
[agent]
  ## Default data collection interval for all inputs
  interval = "$KAFKA_MONITORING_INTERVAL"
  ## Rounds collection interval to 'interval'
  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = true

  ## Telegraf will send metrics to outputs in batches of at most
  ## metric_batch_size metrics.
  ## This controls the size of writes that Telegraf sends to output plugins.
  metric_batch_size = 1000

  ## For failed writes, telegraf will cache metric_buffer_limit metrics for each
  ## output, and will flush this buffer on a successful write. Oldest metrics
  ## are dropped first when this buffer fills.
  ## This buffer only fills when writes fail to output plugin(s).
  metric_buffer_limit = 10000

  ## Collection jitter is used to jitter the collection by a random amount.
  ## Each plugin will sleep for a random time within jitter before collecting.
  ## This can be used to avoid many plugins querying things like sysfs at the
  ## same time, which can have a measurable effect on the system.
  collection_jitter = "0s"

  ## Default flushing interval for all outputs. You shouldn't set this below
  ## interval. Maximum flush_interval will be flush_interval + flush_jitter
  flush_interval = "$KAFKA_MONITORING_INTERVAL"
  ## Jitter the flush interval by a random amount. This is primarily to avoid
  ## large write spikes for users running a large number of telegraf instances.
  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
  flush_jitter = "0s"

  ## By default, precision will be set to the same timestamp order as the
  ## collection interval, with the maximum being 1s.
  ## Precision will NOT be used for service inputs, such as logparser and statsd.
  ## Valid values are "ns", "us" (or "Âµs"), "ms", "s".
  precision = ""

  ## Logging configuration:
  ## Run telegraf with debug log messages.
  debug = true
  ## Run telegraf in quiet mode (error log messages only).
  quiet = false
  ## Specify the log file name. The empty string means to log to stderr.
  logfile = ""

  ## Override default hostname, if empty use os.Hostname()
  hostname = ""
  ## If set to true, do no set the "host" tag in the telegraf agent.
  omit_hostname = false


###############################################################################
#                            OUTPUT PLUGINS                                   #
###############################################################################

# Configuration for influxdb server to send metrics to
[[outputs.influxdb]]
  ## The full HTTP or UDP endpoint URL for your InfluxDB instance.
  ## Multiple urls can be specified as part of the same cluster,
  ## this means that only ONE of the urls will be written to each interval.
  # urls = ["udp://localhost:8089"] # UDP endpoint example
  urls = ["$SM_DB_HOST"] # required
  ## The target database for metrics (telegraf will create it if not exists).
  database = "$SM_DB_NAME" # required

  ## Retention policy to write to. Empty string writes to the default rp.
  retention_policy = ""
  ## Write consistency (clusters only), can be: "any", "one", "quorum", "all"
  write_consistency = "any"

  ## Write timeout (for the InfluxDB client), formatted as a string.
  ## If not provided, will default to 5s. 0s means no timeout (not recommended).
  timeout = "5s"
  username = "$SM_DB_USERNAME"
  password = "$SM_DB_PASSWORD"
  ## Set the user agent for HTTP POSTs (can be useful for log differentiation)
  # user_agent = "telegraf"
  ## Set UDP payload size, defaults to InfluxDB UDP Client default (512 bytes)
  # udp_payload = 512

  ## Optional SSL Config
  # ssl_ca = "/etc/telegraf/ca.pem"
  # ssl_cert = "/etc/telegraf/cert.pem"
  # ssl_key = "/etc/telegraf/key.pem"
  ## Use SSL but skip chain & host verification
  # insecure_skip_verify = false

# # Publish all metrics to /metrics for Prometheus to scrape
# [[outputs.prometheus_client]]
#   ## Address to listen on.
#   listen = ":8096"
#
#   ## Metric version controls the mapping from Telegraf metrics into
#   ## Prometheus format.  When using the prometheus input, use the same value in
#   ## both plugins to ensure metrics are round-tripped without modification.
#   ##
#   ##   example: metric_version = 1; deprecated in 1.13
#   ##            metric_version = 2; recommended version
#   # metric_version = 1
#
#   ## Use HTTP Basic Authentication.
#   basic_username = "$PROMETHEUS_USERNAME"
#   basic_password = "$PROMETHEUS_PASSWORD"
#   ## If set, the IP Ranges which are allowed to access metrics.
#   ##   ex: ip_range = ["192.168.0.0/24", "192.168.1.0/30"]
#   # ip_range = []
#
#   ## Path to publish the metrics on.
#   # path = "/metrics"
#
#   ## Expiration interval for each metric. 0 == no expiration
#   # expiration_interval = "60s"
#
#   ## Collectors to enable, valid entries are "gocollector" and "process".
#   ## If unset, both are enabled.
#   # collectors_exclude = ["gocollector", "process"]
#
#   ## Send string metrics as Prometheus labels.
#   ## Unless set to false all string metrics will be sent as labels.
#   # string_as_label = true
#
#   ## If set, enable TLS with the given certificate.
#   # tls_cert = "/etc/ssl/telegraf.crt"
#   # tls_key = "/etc/ssl/telegraf.key"
#
#   ## Set one or more allowed client CA certificate file names to
#   ## enable mutually authenticated TLS connections
#   # tls_allowed_cacerts = ["/etc/telegraf/clientca.pem"]
#
#   ## Export metric collection time.
#   # export_timestamp = false


###############################################################################
#                            PROCESSOR PLUGINS                                #
###############################################################################

# # Print all metrics that pass through this filter.
# [[processors.printer]]

# ignore metrics with "jvm_" and "jmx_" prefix
[[processors.override]]
  order = 1
  namepass = ["jvm_*", "jmx_*"]
  fielddrop = ["*"]

# java_Memory_HeapMemoryUsage
[[processors.rename]]
  order = 10
  namepass = ["java_Memory_HeapMemoryUsage_init"]

  [[processors.rename.replace]]
    measurement = "java_Memory_HeapMemoryUsage_init"
    dest = "java_Memory_HeapMemoryUsage"

  [[processors.rename.replace]]
    field = "gauge"
    dest = "init"

[[processors.rename]]
  order = 11
  namepass = ["java_Memory_HeapMemoryUsage_used"]

  [[processors.rename.replace]]
    measurement = "java_Memory_HeapMemoryUsage_used"
    dest = "java_Memory_HeapMemoryUsage"

  [[processors.rename.replace]]
    field = "gauge"
    dest = "used"

[[processors.rename]]
  order = 12
  namepass = ["java_Memory_HeapMemoryUsage_committed"]

  [[processors.rename.replace]]
    measurement = "java_Memory_HeapMemoryUsage_committed"
    dest = "java_Memory_HeapMemoryUsage"

  [[processors.rename.replace]]
    field = "gauge"
    dest = "committed"

[[processors.rename]]
  order = 13
  namepass = ["java_Memory_HeapMemoryUsage_max"]

  [[processors.rename.replace]]
    measurement = "java_Memory_HeapMemoryUsage_max"
    dest = "java_Memory_HeapMemoryUsage"

  [[processors.rename.replace]]
    field = "gauge"
    dest = "max"

# java_Memory_NonHeapMemoryUsage
[[processors.rename]]
  order = 20
  namepass = ["java_Memory_NonHeapMemoryUsage_init"]

  [[processors.rename.replace]]
    measurement = "java_Memory_NonHeapMemoryUsage_init"
    dest = "java_Memory_NonHeapMemoryUsage"

  [[processors.rename.replace]]
    field = "gauge"
    dest = "init"

[[processors.rename]]
  order = 21
  namepass = ["java_Memory_NonHeapMemoryUsage_used"]

  [[processors.rename.replace]]
    measurement = "java_Memory_NonHeapMemoryUsage_used"
    dest = "java_Memory_NonHeapMemoryUsage"

  [[processors.rename.replace]]
    field = "gauge"
    dest = "used"

[[processors.rename]]
  order = 22
  namepass = ["java_Memory_NonHeapMemoryUsage_committed"]

  [[processors.rename.replace]]
    measurement = "java_Memory_NonHeapMemoryUsage_committed"
    dest = "java_Memory_NonHeapMemoryUsage"

  [[processors.rename.replace]]
    field = "gauge"
    dest = "committed"

[[processors.rename]]
  order = 23
  namepass = ["java_Memory_NonHeapMemoryUsage_max"]

  [[processors.rename.replace]]
    measurement = "java_Memory_NonHeapMemoryUsage_max"
    dest = "java_Memory_NonHeapMemoryUsage"

  [[processors.rename.replace]]
    field = "gauge"
    dest = "max"

# kafka_controller_KafkaController_ActiveControllerBroker
[[processors.rename]]
  order = 30
  namepass = ["kafka_controller_KafkaController_Value"]
  tagpass = { name = ["ActiveControllerCount"] }

  [[processors.rename.replace]]
    measurement = "kafka_controller_KafkaController_Value"
    dest = "kafka_controller_KafkaController_ActiveControllerBroker"

[[processors.regex]]
  order = 31
  namepass = ["kafka_controller_KafkaController_ActiveControllerBroker"]

  [[processors.regex.tags]]
    key = "broker"
    pattern = "(.*)"
    replacement = "${1}"
    result_key = "value"

[[processors.converter]]
  order = 32
  namepass = ["kafka_controller_KafkaController_ActiveControllerBroker"]

  [processors.converter.tags]
    string = ["value"]

# kafka_server_SessionExpireListener_Value
[[processors.converter]]
  order = 40
  namepass = ["kafka_server_SessionExpireListener_Value"]
  tagpass = { name = ["SessionState"] }

  [processors.converter.tags]
    string = ["value"]

# kafka_server_app_info_Version
[[processors.converter]]
  order = 41
  namepass = ["kafka_server_app_info_Version"]

  [processors.converter.tags]
    string = ["version"]

[[processors.rename]]
  order = 50
  namepass = ["kafka_server_KafkaServer_Value"]
  tagpass = { name = ["BrokerState"] }
  tagexclude = ["name", "url"]

  [[processors.rename.replace]]
    measurement = "kafka_server_KafkaServer_Value"
    dest = "kafka"

  [[processors.rename.replace]]
    field = "gauge"
    dest = "broker_state"

[[processors.rename]]
  order = 51
  namepass = ["kafka_server_ReplicaManager_Value"]
  tagpass = { name = ["PartitionCount"] }
  tagexclude = ["name", "url"]

  [[processors.rename.replace]]
    measurement = "kafka_server_ReplicaManager_Value"
    dest = "kafka"

  [[processors.rename.replace]]
    field = "gauge"
    dest = "partition_count"

[[processors.rename]]
  order = 52
  namepass = ["kafka_server_ReplicaManager_Value"]
  tagpass = { name = ["LeaderCount"] }
  tagexclude = ["name", "url"]

  [[processors.rename.replace]]
    measurement = "kafka_server_ReplicaManager_Value"
    dest = "kafka"

  [[processors.rename.replace]]
    field = "gauge"
    dest = "leader_count"

[[processors.rename]]
  order = 53
  namepass = ["kafka_server_ReplicaManager_Value"]
  tagpass = { name = ["UnderReplicatedPartitions"] }
  tagexclude = ["name", "url"]

  [[processors.rename.replace]]
    measurement = "kafka_server_ReplicaManager_Value"
    dest = "kafka"

  [[processors.rename.replace]]
    field = "gauge"
    dest = "under_replicated_partitions"

[[processors.rename]]
  order = 54
  namepass = ["kafka_server_ReplicaManager_Value"]
  tagpass = { name = ["UnderMinIsrPartitionCount"] }
  tagexclude = ["name", "url"]

  [[processors.rename.replace]]
    measurement = "kafka_server_ReplicaManager_Value"
    dest = "kafka"

  [[processors.rename.replace]]
    field = "gauge"
    dest = "under_min_isr_partition_count"

[[processors.rename]]
  order = 55
  namepass = ["kafka_log_LogCleaner_Value"]
  tagpass = { name = ["DeadThreadCount"] }
  tagexclude = ["name", "url"]

  [[processors.rename.replace]]
    measurement = "kafka_log_LogCleaner_Value"
    dest = "kafka"

  [[processors.rename.replace]]
    field = "gauge"
    dest = "log_cleaner_dead_threads_count"

[[processors.rename]]
  order = 56
  namepass = ["kafka_server_ReplicaFetcherManager_Value"]
  tagpass = { name = ["DeadThreadCount"] }
  tagexclude = ["name", "url", "clientId"]

  [[processors.rename.replace]]
    measurement = "kafka_server_ReplicaFetcherManager_Value"
    dest = "kafka"

  [[processors.rename.replace]]
    field = "gauge"
    dest = "replica_fetcher_dead_threads_count"

# merge all fields
[[aggregators.merge]]
  order = 100
  namepass = ["kafka"]
  period = "$KAFKA_MONITORING_INTERVAL"
  grace = "5s"
  drop_original = true

[[aggregators.merge]]
  order = 101
  namepass = ["java_Memory_HeapMemoryUsage", "java_Memory_NonHeapMemoryUsage"]
  period = "$KAFKA_MONITORING_INTERVAL"
  grace = "5s"
  drop_original = true


###############################################################################
#                            AGGREGATOR PLUGINS                               #
###############################################################################

# # Keep the aggregate min/max of each metric passing through.
# [[aggregators.minmax]]
#   ## General Aggregator Arguments:
#   ## The period on which to flush & clear the aggregator.
#   period = "30s"
#   ## If true, the original metric will be dropped by the
#   ## aggregator and will not get sent to the output plugins.
#   drop_original = false


###############################################################################
#                            INPUT PLUGINS                                    #
###############################################################################

# Read metrics from one or many prometheus clients
[[inputs.prometheus]]
  ## An array of urls to scrape metrics from.
  urls = [$PROMETHEUS_URLS]

  ## Metric version controls the mapping from Prometheus metrics into
  ## Telegraf metrics.  When using the prometheus_client output, use the same
  ## value in both plugins to ensure metrics are round-tripped without
  ## modification.
  ##
  ##   example: metric_version = 1;
  ##            metric_version = 2; recommended version
  # metric_version = 1

  ## Url tag name (tag containing scrapped url. optional, default is "url")
  # url_tag = "url"

  ## Use bearer token for authorization. ('bearer_token' takes priority)
  # bearer_token = "/path/to/bearer/token"
  ## OR
  # bearer_token_string = "abc_123"

  ## HTTP Basic Authentication username and password. ('bearer_token' and
  ## 'bearer_token_string' take priority)
  username = "$KAFKA_USER"
  password = "$KAFKA_PASSWORD"

  ## Specify timeout duration for slower prometheus clients (default is 3s)
  response_timeout = "10s"

  ## Optional TLS Config
  # tls_ca = /path/to/cafile
  # tls_cert = /path/to/certfile
  # tls_key = /path/to/keyfile

  ## Use TLS but skip chain & host verification
  # insecure_skip_verify = false

# Read metrics from one or more commands that can output to stdout
[[inputs.exec]]
  ## Commands array
  commands = [
    "python3 /opt/kafka-monitoring/exec-scripts/kafka_metric.py"
  ]

  ## Timeout for each command to complete.
  timeout = "$KAFKA_EXEC_PLUGIN_TIMEOUT"

  ## Data format to consume.
  ## Each data format has it's own unique set of configuration options, read
  ## more about them here:
  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
  data_format = "influx"

###############################################################################
#                            SERVICE INPUT PLUGINS                            #
###############################################################################
